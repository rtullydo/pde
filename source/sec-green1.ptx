<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-generalized-functions-and-m-delta-m-functions">
  <title>Generalized functions and <m>\delta</m> functions</title>
  <subsection xml:id="subsec-linear-algebraic-motivation">
    <title>Linear algebraic motivation</title>
    <p>
      The basic idea of the theory of Green's functions is to construct complete solutions out of superpositions of fundmental solutions. To get the thematic idea at play, consider the linear algebraic equation
      <me>
        A \mathbf{u} = \mathbf{f}.
      </me>
      In this example, imagine that the system describes a system of masses interconnected by springs (which is recorded by the matrix <m>A</m>) where the <m>i</m>th entry of <m>\mathbf u</m> is the displacement <m>u_i</m> of the <m>i</m>th mass from equilibrium, and the entries of <m>\mathbf f</m> describe the force <m>f_i</m> applied to the <m>i</m>th mass. 
    </p>

    <p>Let <m>\mathbf{e}_i</m> denote the <m>i</m>th elementary basis vector. We can interpret <m>\mathbf{e}_i</m> as representing a <em>unit impulse concentrated at mass <m>i</m>.</em> Then applying that force to the system gives
    <me>
      A \mathbf{u}_i = \mathbf{e}_i
    </me>
    so that <m>\mathbf u_i = A\inv \mathbf{e}_i</m>, which is the vector that records the <em>response</em> of the system to <m>\mathbf{e}_i</m>. The key observation is that any force vector can be decomposed into a superposition of impulses <m>\mathbf{f} = \sum f_i \mathbf{e}_i</m> and the corresponding response to <m>f</m> can be written as a superposition of responses to impulses <m>\mathbf{u} = \sum f_i \mathbf{u}_i</m>. This is the same gameplan we'll pursue in the context of linear differential equations - find the response to concentrated unit impulses, and then superimpose those responses to get a general solution.
    </p>
  </subsection>

  <subsection xml:id="subsec-delta-functions">
    <title>Delta functions</title>
    <p>
      Suppose that we're trying to solve a linear boundary value problem on <m>a \lt x \lt b</m>. The unit impulse at a point <m>x = \xi</m> is given by the <q>delta function</q>, <m>\delta_\xi(x)</m>.
      <ol>
        <li>
          <p>
            In order to concentrate force at a single point, we require that 
            <me>
              \delta_\xi(x) = 0 \,\,\, \text{ if } x \neq \xi.
            </me>
          </p>
        </li>
        <li>
          <p>
            In order to ensure that the total force is of magnitude 1, we require that
            <me>
              \int_a^b \delta_\xi(x) \, dx = 1 \,\,\, \text{ if } \xi \in (a,b).
            </me>
          </p>
        </li>
      </ol>
      Of course, these are inconsistent conditions - a function that is identically 0 except at a single point must have integral 0 in the standard Riemann theory. So <m>\delta_\xi</m> must be something else, something <em>not a function</em>! Instead, <m>\delta_\xi</m> belongs to a broader family of mathematical objects called <em>distributions</em> or <em>generalized functions</em>. In fact, the delta function would more properly be called the delta distribution. However, tradition and usage lead us to continue with the usual naming convention.</p>
      
    <p>  
      There are two typical intuituve ways to understand the <m>\delta</m> function. Both have value, but neither is precise. The first is almost certainly more familiar. 
    </p>

    <warning>
      <p>
        The following discussion is a heuristic, not a rigorous treatment - however, in taking this approach we are following in the footsteps of Cauchy, Green, and Heaviside, who developed and used this machinery well before it was put on firm mathematical footing. Heaviside once said <q>Shall I refuse my dinner because I do not understand the process of digestion?</q> We too shall eat.
      </p>
    </warning>

     <p>  
      The first method is to construct <m>\delta</m> as a limit of appropriately chosen functions. We've already seen such a method in the context of the heat kernel. Here is another family of functions that leads to the delta function. Let 
      <me>
        g_n(x) = \frac{n}{\pi(1 + n^2 x^2)}
      </me>
      be a family of functions on <m>\R</m>. Then the pointwise limit of <m>g_n</m> is
      <me>
        \lim_{n\to \infty} g_n(x) = \begin{cases} 0 \amp x \neq 0 \\ \infty \amp x = 0 \end{cases},
      </me>
      and
      <me>
        \int_{-\infty}^\infty g_n(x)\, dx = 1 \text{ for all } n.
      </me>
    </p>
    <definition xml:id="def-delta">
      <statement>
        <p>
          The delta function <m>\delta(x) = \delta_0(x)</m> is given by
        <me>
          \delta(x) = \lim_{n \to \infty} g_n(x).
        </me>
        The delta function at <m>x = \xi</m> is given by
        <me>
          \delta_\xi(x) = \delta(x - \xi).
        </me>
        </p>
      </statement>
    </definition>
    <p>
      The previous definition isn't quite rigorous in some fundamental ways. For example, we <em>still</em> have the issue that 
      <me>
        \lim_{n\to \infty} \int_\R g_n(x)\, dx =  1 \neq \int_\R \lim_{n \to \infty} g_n(x) \, dx = 0.
      </me>
      Moreover, <m>\delta_\xi</m> defined this way has no meaningful value at <m>x = \xi</m>. 
    </p>

    <p>
      The second approach we'll take is closer to the real story. We'll leverage some ideas from inner product spaces (the study of spaces of functions by way of linear algebra is called <em>functional analysis</em>). Let <m>u(x)</m> be a continuous function on <m>(a,b)</m>. Assume that <xref ref="def-delta"/> makes sense. Then
      <me>
        \underbrace{\int_a^b \delta_\xi(x) u(x) \, dx = \int_a^b \delta_\xi(x) u(\xi) \, dx}_{\text{ as } \delta_\xi = 0 \text{ everywhere else}} = u(\xi)\int_a^b \delta_\xi(x) \, dx = u(\xi).
      </me>
      Define a map <m>L_\xi:C^0[a,b] \to \R</m> by
      <me>
        L_\xi(u) = \int_a^b \delta_\xi(x) u(x) \, dx = u(\xi).
      </me>
      Because <m>L</m> is defined by an integral, it is a linear map, in the sense that
      <me>
        L_\xi(\alpha u + \beta v) = \alpha L_\xi(u) + \beta L_\xi(v).
      </me>
      A linear map acting on a vector space and mapping into scalars is called a <em>linear functional</em>. In fact, every function <m>g \in \L^1(a,b)</m> defines a linear functional <m>L</m> via <m>L:f \mapsto \int_a^b f(x) g(x) \, dx</m>. We can record this information using the <m>\L^2</m> inner product notation <m>L: f \mapsto \ip{f}{g}</m>. Restating our previous discussion in these terms, every function <m>g \in \L^1(a,b)</m> induces a linear functional <m>L</m> by
      <me>
        L[f] = \ip{f}{g}.
      </me>
    </p>

    <p>
      In finite dimensional vector spaces, the converse is true - that is, every linear functional <m>L: \R^n \to \R</m> has a representing vector <m>y \in \R^n</m> such that
      <me>
        L[x] = \ip{x}{y} \, \text{ for all } x \in \R^n.
      </me>
      This is a much trickier statement in infinite dimensional settings, and yet we can proceed formally. Certainly, the evaluation map
      <me>
        L_\xi[u] = u(x) 
      </me>
      is a linear functional since 
      <me>
        L_\xi[\alpha u + \beta v] = (\alpha u + \beta v)(\xi) = \alpha u(\xi) + \beta v(\xi) = \alpha L_\xi[u] + \beta L_\xi[v].
      </me>
      If <m>L_\xi</m> was to have a representing vector <m> y \in \L^1</m> so that 
      <me>
        L[u] = \ip{u}{y},
      </me>
      then our previous discussion about the delta function would give that <m> y = \delta_\xi</m> satifies the requirement. The existence of such a vector is a non-trival theorem, and requires that we work in the <em>dual space</em> of <m>\L^1</m>, denoted <m>\L^1(a,b)\ad</m>, which is the space of linear functionals acting on <m>\L^1</m>. We'll spend more time talking about dual spaces when we introduce the notions of weak derivatives and weak convergence in the next section. For now, let us continue our discussion of the delta function. 
      
    </p>
  </subsection>
  <subsection xml:id="subsec-properties-of-the-delta-function">
    <title>Properties of the delta function</title>
    <p>
      We will interchange between both methods of understanding <m>\delta_\xi</m> as befits our current exploration. First, then, let us note that we can form linear combinations of delta functions. Let <m> h = \sum_i c_i \delta_{\xi_i}</m>. Then 
      <me>
        \ip{h}{u} = \ip{\sum_i c_i \delta_{\xi_i}}{u} = \sum_i c_i \ip{\delta_{\xi i}}{u} = \sum_i c_i u(\xi_i).
      </me>
    </p>

    <p>
      Second, we can integrate <m>\delta</m>. One approach to discovering the formula is to integrate <m>g_n</m> and take a limit. Indeed,
      <me>
        \int g_n(x) \, dx = \int \frac{n}{\pi(1 + n^2 x^2)}\, dx = \frac{1}{\pi} \tan\inv(nx) + C.
      </me>
      Let us choose <m>C = 1/2</m> (this is a compatibility condition with Fourier series). Then 
      <me>
        \lim_{n \to \infty} \frac{1}{\pi} \tan\inv(nx) + \frac{1}{2} = \sigma(x) = \begin{cases} 0 \amp x \lt 0 \\ 1 \amp x \gt 0 \\ \frac{1}{2} \amp x = 0 \end{cases}.
      </me>
      That is, the integral of <m>\delta</m> is the unit step function <m>\sigma</m>. Then, motivated by the FTC for functions, we declare that 
      <me>
        \frac{d \sigma}{d x} = \delta,
      </me>
      which gives a method that allows differentiation to extend to discontinuous functions.
    </p>

    <p>
      The derivative above is actually an instance of the <em>distributional derivative</em>. Let us become more familiar with this type of differentiation by looking at the derivative of <m>\delta</m> itself. Assume that <m>0 \in (a,b)</m>. First, we use the identification
      <me>
        \ip{\delta'}{u} = \int_a^b \delta'(x) u(x) \, dx.
      </me>
      If we could swap out the <m>\delta'</m> for a <m>\delta</m>, we'd be in business. Fortunately, this is precisely what integration by parts will allow us to do. Formally,
      <md>
        <mrow>\ip{\delta'}{u} \amp= \int_a^b \delta'(x) u(x)</mrow>
        <mrow> \amp= u(x) \delta(x)\vert_a^b - \int_a^b \delta(x) u'(x) \, dx </mrow>
        <mrow> \amp= u(b)\delta(b) - u(a)\delta(a) - \ip{\delta}{u'} </mrow>
        <mrow> \amp= -u'(0). </mrow>
      </md>
      Hence we define <m>\delta'</m> to be the distribution so that for all appropriate <m>u</m>, we have
      <me>
        \ip{\delta'}{u} = \ip{\delta}{-u'} = -u'(0).
      </me>
    </p>
    <warning>
      <p>We're going to play slightly loose with the distinction between <em>weak</em> and <em>distributional</em> ideas in this discussion. If you're interested in digging deeper on the distinction, you should consult a graduate level analysis text like Folland's <em>Modern Analysis</em>.
      </p>
    </warning>
    <p>
      When we write <m>\delta'</m>, we are referring to a weak derivative, as <m>\delta</m> is not a function, and hence does not have a functional derivative. 
    </p>
    <definition xml:id="def-weak-derivative">
      <statement>
        <p>
          Suppose that <m>u \in \L^1(a,b)</m>. A function <m>v \in \L^1(a,b)</m> is called a <em>weak derivative</em> of <m>u</m> if 
          <me>
            \int_a^b u(x) \phi'(x) \, dx = -\int_a^b v(x) \phi(x) \, dx
          </me>
          for all <m>C^\infty</m> functions <m>\phi</m> with <m>\phi(a) = \phi(b) = 0</m>.
        </p>
        <!--<p>
          More generally, if <m>u</m> is a distribution, then <m>v</m> is a distributional derivative of <m>u</m> if 
          <me>
            \ip{u}{\phi'} = -\ip{v}{\phi}.
          </me>
          for all test functions <m>\phi</m>. 
        </p>-->
      </statement>
    </definition>
    <p>
      Generally, we use the adjective <em>weak</em> to describe behavior that happens in an inner product. Earlier, we claimed that we could take <m>n \to \infty</m> to understand the delta function as
      <me>
        g_n(x) = \frac{n}{\pi(1+n^2 x^2)} \to \delta(x).
      </me>
      This limit makes no sense as it does not converge at all at <m>0</m> and the limiting object isn't even a function. What we really meant was that
      <me>
        \ip{g_n}{u} \to \ip{\delta}{u} \, \text{ for all integrable } u ;  
      </me>
      that is, the linear functionals represented by <m>g_n</m> converge to the linear functional of evaluation at <m>x=0</m>. This is an example of what we call <em>weak convergence</em>. 
    </p>
    <definition xml:id="def-weak-convergence">
      <statement>
        <p>
          A sequence <m>\{ f_n \}</m> converges weakly to <m>f</m> if 
          <me>
            \lim_{n \to \infty} \ip{f_n}{u} = \ip{f}{u} \, \text{ for all appropriate } u.
          </me>
          In this case we write <m>f_n \xrightarrow{wk} f</m>. 
        </p>
      </statement>
    </definition>
    <p>
      As another example, consider the the vector space <m>L^2[0, 2\pi]</m> and the sequence <m>f_n(x) = \sin nx</m>. This sequence has no pointwise limit at all, nor does it converge in <m>L^2</m>. However, <m>u\in L^2[0,2\pi]</m>, 
      <md>
        <mrow>\lim_{n \to \infty} \ip{f_n}{u} \amp \lim_{n \to \infty} \int_0^{2\pi} \sin nx \, u(x)\, dx </mrow>
        <mrow> \amp= 0 </mrow>
        <mrow> \amp= \ip{0}{u}. </mrow>
      </md>
      Hence, <m>f_n \xrightarrow{wk} 0 </m>. That the limit of the integral above tending to 0 is a consequence of the <url href="https://en.wikipedia.org/wiki/Riemann%E2%80%93Lebesgue_lemma">Riemann-Lebesgue Lemma</url>.
    </p>

    <p>
      Now equipped with a notion of convergence appropriate to working with the delta function, let us try to produce a Fourier series for <m>\delta</m>. Let us consider <m>L^2[-\pi, \pi]</m>. The Fourier series representation for <m>\delta</m> should take the form
      <me>
        \delta \sim \frac{1}{2\pi} + \frac{1}{\pi} \sum_{n=1}^\infty A_n \cos nx + B_n \sin nx,
      </me>
      where
      <me>
        A_n = \frac{1}{\pi}\int_{-\pi}^{\pi} \delta(x) \cos nx \, dx = \frac{1}{\pi} \cos 0 = \frac{1}{\pi},
      </me>
      and
      <me>
        B_n = \frac{1}{\pi}\int_{-\pi}^{\pi} \delta(x) \sin nx \, dx = 0.
      </me>
      Hence,
      <me>
        \delta(x) \sim \frac{1}{2\pi} + \frac{1}{\pi} \sum_{n=1}^\infty \cos nx.
      </me>
      Since <m>\delta</m> isn't a function, the Fourier series can't converge to <m>\delta</m> pointwise or in <m>L^2</m>. However, if we denote <m>S_N = \frac{1}{2\pi} + \frac{1}{\pi} \sum_{n=1}^N \cos nx</m>, then
      <me>
        \ip{S_N}{u} = \ip{\frac{1}{2\pi} + \frac{1}{\pi} \sum_{n=1}^N \cos nx}{u} \to \ip{\delta}{u} \, \text{ for all } u \in L^2[-\pi,\pi].
      </me>
      Hence <m>S_n \xrightarrow{wk} \delta</m>.
    </p>
  </subsection>
</section>